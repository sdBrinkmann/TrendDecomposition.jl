var documenterSearchIndex = {"docs":
[{"location":"man/penalized/#Penalized-Smoothing","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"","category":"section"},{"location":"man/penalized/","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"The term penalized smoothing here denotes a class of smoothers  that minimize a criterion  subject to a penalizing term. The methods introduced here are in fact one of the earliest uses in statistics of a  penalizing term first introduced in 1898 by George Bohlmann[Bohl99] and its  the first smoothing procedure posed as an optimization problem. ","category":"page"},{"location":"man/penalized/","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"[Bohl99]: Bohlmann, G. (1899). Ein Ausgleichungsproblem. Nachrichten von der Gesellschaft der Wissenschaften zu Göttingen, Mathematisch-Physikalische Klasse, 1899, 260-271.","category":"page"},{"location":"man/penalized/","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"Given our time series y, it is assumed that it can be decomposed into","category":"page"},{"location":"man/penalized/","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"\ty_t = tau_t + c_t","category":"page"},{"location":"man/penalized/","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"where tau_t is the trend component and c_t is the cycle component at time t.  The criterion we want to minimize is the squared difference between the  trend component and the corresponding data point (y_t - tau_t)^2.","category":"page"},{"location":"man/penalized/","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"For the penalizing term we differentiate between the traditionally used l_2 norm and more novel approaches using the l_1 norm similar to L1 and L2 regularization in regression analysis. ","category":"page"},{"location":"man/penalized/#Whittaker-Henderson-Smoothing","page":"Penalized Smoothing","title":"Whittaker-Henderson Smoothing","text":"","category":"section"},{"location":"man/penalized/","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"Using the squared m'th difference of tau_t, written here as (Delta^m tau_t)^2, as penalizing term and lambda  0 as regularization parameter, the trend component can be estimated as solution to the optimization problem:","category":"page"},{"location":"man/penalized/","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"^tau_t = \toperatorname*argmin_tau Bigg sum_t=1^n (y_t - tau_t)^2 + lambda sum_t=m+1^n \n(Delta^m tau_t)^2Bigg","category":"page"},{"location":"man/penalized/","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"All functions below, bohlmannFilter, hpFilter, bhpFilter, use the algebraic solution to the minimization problem.  The lambda value has to be selected beforehand by the user. For lambda to 0 the solution converges to the original data and for lambda to infty the trend becomes a straight line. ","category":"page"},{"location":"man/penalized/#TrendDecomposition.bohlmannFilter","page":"Penalized Smoothing","title":"TrendDecomposition.bohlmannFilter","text":"bohlmannFilter(x :: Vector, m :: Int, λ :: Real)\n\nThis is the generalization of the Hodrick-Prescott filter, also known as Whittaker-Henderson smoothing, using the m-th difference to estimate the trend component.\n\n\n\n\n\n","category":"function"},{"location":"man/penalized/#Hodrick-Prescott-(HP)-Filter","page":"Penalized Smoothing","title":"Hodrick-Prescott (HP) Filter","text":"","category":"section"},{"location":"man/penalized/","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"The case using second difference with m = 2 was proposed by Leser (1961)[Leser61] for trend estimation and the above equation becomes ","category":"page"},{"location":"man/penalized/","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"[Leser61]: Leser, C. E. V. (1961). A Simple Method of Trend Construction. Journal of the Royal Statistical Society. Series B (Methodological), 23(1), 91–107.","category":"page"},{"location":"man/penalized/","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"^tau_t = \toperatorname*argmin_tau Bigg sum_t=1^n (y_t - tau_t)^2 + lambda sum_t=m+1^n \n(tau_t+1 - 2 tau_t + tau_t-1)^2Bigg","category":"page"},{"location":"man/penalized/","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"In the literature this procedure is also known as the Hodrick-Prescott filter.  In general, the following parameters are recommended for lambda","category":"page"},{"location":"man/penalized/","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"Basis Period length λ\nAnnual 1 100\nQuarterly 4 1600\nMonthly 12 14,600","category":"page"},{"location":"man/penalized/#TrendDecomposition.hpFilter-Tuple{Vector, Real}","page":"Penalized Smoothing","title":"TrendDecomposition.hpFilter","text":"hpFilter(x::Vector, λ::Real)\n\nApply the Hodrick-Prescott decomposition to vector x with multiplier value λ.\n\nFunction returns the trend component.\n\n\n\n\n\n","category":"method"},{"location":"man/penalized/#Boosted-HP-Filter","page":"Penalized Smoothing","title":"Boosted HP Filter","text":"","category":"section"},{"location":"man/penalized/","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"In order to provide a better trend estimation, according to Philips and Shi (2021)[PhilShi21] the HP Filter can be applied repeatedly over the cycle estimate(s). This idea is akin to L_2-boosting in machine learning, hence the method is called boosted HP (bHP) filter. ","category":"page"},{"location":"man/penalized/","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"[PhilShi21]: Phillips, P.C.B. and Shi, Z. (2021), BOOSTING: WHY YOU CAN USE THE HP FILTER. International Economic Review, 62: 521-570.","category":"page"},{"location":"man/penalized/","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"hpFilter(x::Vector, λ::Real, iter::Int) allows the repeated application of the HP filter by a fixed ammount of times determind by the parameter iter.","category":"page"},{"location":"man/penalized/#TrendDecomposition.hpFilter-Tuple{Vector, Real, Int64}","page":"Penalized Smoothing","title":"TrendDecomposition.hpFilter","text":"hpFilter(x::Vector, λ::Real, iter::Int)\n\nCompute boosted Hodrick-Prescott filter with number of iterations specified by iter.\n\nFunction returns the trend component.\n\n\n\n\n\n","category":"method"},{"location":"man/penalized/","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"bhpFilter implements an iterative approach, where a stopping criterion determines the ammout of iterations.","category":"page"},{"location":"man/penalized/#TrendDecomposition.bhpFilter","page":"Penalized Smoothing","title":"TrendDecomposition.bhpFilter","text":"bhpFilter(x::Vector, λ::Real; Criterion=\"BIC\", max_iter::Int = 100, p::Float64=0.05)\n\nComputes the boosted Hodrick-Prescott filter by appyling the filter iteratively over the trend component with stop criterion being either a Bayesian-type information criterion (BIC) or an augmented Dickey-Fuller (ADF) test.\n\nFunction returns the trend component.\n\n\n\n\n\n","category":"function"},{"location":"man/penalized/#l_1-Trend-Filtering","page":"Penalized Smoothing","title":"l_1 Trend Filtering","text":"","category":"section"},{"location":"man/start/#Get-Started","page":"Get Started","title":"Get Started","text":"","category":"section"},{"location":"man/start/","page":"Get Started","title":"Get Started","text":"This package is now featured on the official general Julia package registry.  Simply use Julia's package manager pkg to add TrendDecomposition to your preferred environment.","category":"page"},{"location":"man/start/","page":"Get Started","title":"Get Started","text":"@(v1.11) pkg> add TrendDecomposition\n\njulia> using TrendDecomposition","category":"page"},{"location":"man/start/","page":"Get Started","title":"Get Started","text":"The developing branch of this package can either be employed  by cloning this repository or by using the Julia package manager. With the package manager simply use the add command:","category":"page"},{"location":"man/start/","page":"Get Started","title":"Get Started","text":"@(v1.11) pkg> add https://github.com/sdBrinkmann/TrendDecomposition.jl","category":"page"},{"location":"man/start/","page":"Get Started","title":"Get Started","text":"warning: Warning\nThis package is currently under rapid development and follows Semantic Versioning. Until the 1.0.0 release is reached, the API of this package can change with any minor version update,  please  consult the documentation of this package after each update when using this package.","category":"page"},{"location":"man/start/#Usage","page":"Get Started","title":"Usage","text":"","category":"section"},{"location":"man/start/","page":"Get Started","title":"Get Started","text":"The basic usage of trend estimation using the Hodrick-Prescott fitler is demonstrated with the US industrial production index (IPI) provided by FRED data service.","category":"page"},{"location":"man/start/","page":"Get Started","title":"Get Started","text":"using TrendDecomposition\nusing CSV\n\n# Set path to directory where time series is located\npath = \"/.../data\"\n\nIPI = CSV.read(\"$(path)/IPB50001SQ.csv\", copycols=true)\n\n# HP filter with λ = 1600\nhp = hpFilter(IPI[!, 2], 1600)\n\n# The above is equivalent to Whittaker-Henderson smoothing with m = 2 differentiation\nwh = bohlmannFilter(IPI[!, 2], 2, 1600)\n\n# Boosted HP filter with baysian-type information criterion (BIC)\nbHP_bic = bhpFilter(IPI[!, 2], 1600, Criterion=\"BIC\")\n\n# Boosted HP filter with augmented Dickey-Fuller (ADF) test \nbHP_adf = bhpFilter(IPI[!, 2], 1600, Criterion=\"ADF\", p=0.01)","category":"page"},{"location":"man/start/","page":"Get Started","title":"Get Started","text":"(Image: HP Results)","category":"page"},{"location":"man/start/#Classical-decomposition","page":"Get Started","title":"Classical decomposition","text":"","category":"section"},{"location":"man/start/","page":"Get Started","title":"Get Started","text":"This package implements moving averages with rollingAverage and seasonal averages with maSeason. With the help of both functions, we can conduct time series decompositions into a trend, seasonal and noise component. The function maDecompose replicates the decompose{stats} functionallity implemented in R.","category":"page"},{"location":"man/start/","page":"Get Started","title":"Get Started","text":"using Plots\nusing TrendDecomposition\n\n# Kendall M. G., Stuard A. (1983). The Advanced Theory of Statistics, Vol. 3\nx = [-50, 175, 149, 214, 247, 237, 225, 329, 729, 809,\n       530, 489, 540, 457, 195, 176, 337, 239, 128, 102, 232, 429, 3,\n     98, 43, -141, -77, -13, 125, 361, -45, 184]\n\n\nres = maDecompose(x, 4, combine=true)\n\nplot(res; layout = (4, 1), title=[\"y\" \"trend\" \"season\" \"noise\"], legend=false, tickfontsize=4)\n","category":"page"},{"location":"man/start/","page":"Get Started","title":"Get Started","text":"(Image: Classical Decomposition)","category":"page"},{"location":"man/start/#Holt-Winters-Method","page":"Get Started","title":"Holt-Winters Method","text":"","category":"section"},{"location":"man/start/","page":"Get Started","title":"Get Started","text":"For both local trend and seasonal component estimation, the Holt-Winters method can be used. The two following examples illustrate its dual usecase of modeling a trend and seasonal component for both decomposition and for forecasting time series.","category":"page"},{"location":"man/start/#Decomposition","page":"Get Started","title":"Decomposition","text":"","category":"section"},{"location":"man/start/","page":"Get Started","title":"Get Started","text":"using Plots\nusing CSV\nusing DataFrames\n\nusing TrendDecomposition\n\n# Box, G.E.P., Jenkins, G.M. and Reinsel, G.C. (1994) Time Series Analysis; Forecasting and Control. 3rd Edition\nair = CSV.read(\"./AirPassengers.csv\", DataFrame, copycols=true)\ndata = air[!, 2]\n\nseasons = 12\n\n# Decomposition using multiplicative model with automatic optimization\nf1, D1, p1 = holtWinters(data, seasons, model=:mul)\n\n# difference between data points and estimated values\nresidual = data .- f1\n\nplot([data D1 residual], layout = (5, 1), ylabel = [\"y\" \"level\" \"slope\" \"season\" \"residual\"], legend=false, tickfontsize=4, color=:black, guidefontsize=8)\n","category":"page"},{"location":"man/start/","page":"Get Started","title":"Get Started","text":"(Image: HW Decomposition)","category":"page"},{"location":"man/start/#Forecasting","page":"Get Started","title":"Forecasting","text":"","category":"section"},{"location":"man/start/","page":"Get Started","title":"Get Started","text":"The damping factor varphi can be typed in julia with the keyboard as follows: \\varphi<tab>","category":"page"},{"location":"man/start/","page":"Get Started","title":"Get Started","text":"\n# Forecast horizon h = 24 and damping factor \\varphi = 0.95\nf2, D2, p2 = holtWinters(data, seasons, h=24, model=:mul, φ = 0.95)\n\nn = length(data)\nsteps = (n+1):(n+24)\n\nplot(data, color=:black, legend=false, title=\"HW forecast h=24\")\nplot!(steps, f2[(end-23):end], color=:red, label=\"forecast\")","category":"page"},{"location":"man/start/","page":"Get Started","title":"Get Started","text":"(Image: HW forecasting)","category":"page"},{"location":"man/exponential/#Exponential-Smoothing","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"","category":"section"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Exponential smoothing, also known as exponentially weighted moving average (EWMA), allows us to model a time series as an additive or multiplicative composition, in order to conduct forecasts. Here in its additive form it can be written as ","category":"page"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"\ty_t = l_t + S_t + u_t","category":"page"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"where l denotes the level or trend component and S the seaonal component, the  latter component is only included in the Holt-Winters method. ","category":"page"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"The residual or noise term u_t is implicitly assumed and the parameters of the  models down below are estimated by minimizing the squared residual terms, ^u_t = y_t - ^y_t,  summed over the entire time series. The model makes otherwise no assumptions and is stated in its classic recursive fashion.","category":"page"},{"location":"man/exponential/#Simple-exponential-smoothing","page":"Exponential Smoothing","title":"Simple exponential smoothing","text":"","category":"section"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Simple exponential smoothing is a special case of weighted moving averages, where weights decline exponentially. Implemented as a recursion it is know as  exponentially weighted moving average (EWMA) defined as:","category":"page"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"    l_t = (1-lambda) * l_t-1 + lambda * y_t","category":"page"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"For the recursion to work a start value has to be defined. Given that the time series y_t is recorded for t = 1T, a start value y_0 has to be selected.","category":"page"},{"location":"man/exponential/#TrendDecomposition.expSmoothing","page":"Exponential Smoothing","title":"TrendDecomposition.expSmoothing","text":"expSmoothing(y :: Vector, λ :: Real; startValue::Bool = true, v_0::Real = 0.0)\n\nSimple exponential smoothing with smoothing factor λ. If startValue equals true, v_0 will be used as initial value, otherwise the first element of y (y[1]) will be selected as starting value. Using y[1] as starting value will result in a different mathematical function.   \n\n\n\n\n\n","category":"function"},{"location":"man/exponential/#Holt's-procedure","page":"Exponential Smoothing","title":"Holt's procedure","text":"","category":"section"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"In order to account for a trend and to make forecasts a local slope b_t was introduced by Holt (1957)[Holt57] , which is updated each period t.  As for most time series it is unrealistic to assume that a linear line is to presist much long into the future, a damping factor varphi (\\varphi) can be used and is set between 0  varphi  1.","category":"page"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"[Holt57]: Holt, Charles C. (1957). \"Forecasting Trends and Seasonal by Exponentially Weighted Averages\". Office of Naval Research Memorandum","category":"page"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"\tbeginaligned\n    l_t = lambda_1 * y_t + (1-lambda_1) * (l_t-1 + varphi b_t-1) \n\tb_t = lambda_2 * (l_t + l_t-1) + (1-lambda_2) * varphi b_t-1 \n\t^y_t = l_t-1 + varphi b_t -1\n\tendaligned","category":"page"},{"location":"man/exponential/#TrendDecomposition.holtLinear-Tuple{Vector, Real, Real}","page":"Exponential Smoothing","title":"TrendDecomposition.holtLinear","text":"holtLinear(y :: Vector, λ₁ :: Real, λ₂ :: Real;\n                startValues::Tuple=(), h::Int = 0, φ::Real = 1.0)\n\nHolt's linear trend method with smoothing factors λ₁ (level) and λ₂ (slope) and damping factor φ (\\varphi) given vector y (N x 1) and h forecast periods.\n\nStart values of level and slope can be given as tuple (l₀, b₀), otherwise output values start at period 3.\n\nReturns a tuple with (f, D), where f is a ((N+h) x 1) vector with the forecast values and D is a (N x 2) matrix with the level and slope as columns.\n\n\n\n\n\n","category":"method"},{"location":"man/exponential/#Double-exponential-smoothing","page":"Exponential Smoothing","title":"Double exponential smoothing","text":"","category":"section"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"By using the above introduced residual or error term ^u_t, the level l_t and slope b_t parameters can be estimated as:","category":"page"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"\tbeginaligned\n\tl_t = l_t-1 + varphi b_t-1 + (1 - lambda^2) ^u_t \n\tb_t = varphi b_t-1 + (1 - lambda)^2 ^u_t  \n\t^y_t = l_t-1 + varphi b_t -1\n\tendaligned","category":"page"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"This recursion is known as double exponential smoothing and also named after R.G. Brown and can been seen as a special case of Holt's procedure by setting lambda_0 = 1 - lambda^2 and lambda_1 = frac1 - lambda1 + lambda.","category":"page"},{"location":"man/exponential/#TrendDecomposition.brownLinear-Tuple{Vector, Real}","page":"Exponential Smoothing","title":"TrendDecomposition.brownLinear","text":"brownLinear(y :: Vector, λ :: Real; h::Int = 0, startValues::Tuple=(), φ::Real = 1.0)\n\nDouble exponential smoothing with smoothing factor λ and damping factor φ (\\varphi) given vector y (N x 1) and h forecast periods.\n\nStart values of level and slope can be given as tuple (l₀, b₀), otherwise output values start at period 3.\n\nReturns a tuple with (f, D), where f is a (N+h) x 1 vector with the forecast values and D is a (N x 2) matrix with the level and slope as columns.\n\n\n\n\n\n","category":"method"},{"location":"man/exponential/#Holt-Winters-forecasting","page":"Exponential Smoothing","title":"Holt-Winters forecasting","text":"","category":"section"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"In addition a seasonal component S_t can be modeled for number of seasons s. This introduces an additional equation for S_t: ","category":"page"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"\tbeginaligned\n    l_t = lambda_1 * (y_t - S_t-s) + (1-lambda_1) * (l_t-1 + varphi b_t-1) \n\tb_t = lambda_2 * (l_t + l_t-1) + (1-lambda_2) * b_t-1 varphi \n    S_t = lambda_3 * (y_t + l_t) + (1-lambda_3) * S_t-s \n\t^y_T+h = l_T + b_T (varphi + varphi^2 ++ varphi^h) + S_T+h-s\n\tendaligned","category":"page"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"As for the t=1,...,s first values there exists no previous values for S_t,  they have to be estimated before starting the recursion computation.  maSeason will be used for the first 2*s values of the detrended time series.  Similar for forecast horizions h > s, the same latest s estimated season components from time periods (T-s-1) to T have to be used over again.","category":"page"},{"location":"man/exponential/#TrendDecomposition.holtWinters-Tuple{Vector, Real, Real, Real, Int64}","page":"Exponential Smoothing","title":"TrendDecomposition.holtWinters","text":"holtWinters(y :: Vector, λ₁ :: Real, λ₂ :: Real, λ₃ :: Real, s::Int;\n                 startValues::Tuple=(), h::Int = 0, model=:add, φ::Real = 1.0)\n\nHolt-Winters method with smoothing factors λ₁ (level), λ₂ (slope) and λ₃ (season) and damping factor φ (\\varphi) given vector y (N x 1) and h forecast periods.\n\nThe number of seasons s has to be specified (s>0), else use TrendDecomposition.holtLinear. \n\nStart values of level and slope can be given as tuple (l₀, b₀), otherwise output values start at period 3.\n\nReturns a tuple with (f, D), where f is a ((N+h) x 1) vector with the forecast values and D is a (N x 3) matrix with the level, slope, season values as columns.\n\n\n\n\n\n","category":"method"},{"location":"man/exponential/#General-behavior-of-holtLinear,-brownLinear-and-holtWinters","page":"Exponential Smoothing","title":"General behavior of holtLinear, brownLinear and holtWinters","text":"","category":"section"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"By default varphi=1 in holtLinear, brownLinear and holtWinters method implementations, so that no damping takes place unless explicitly set. If no start values (l_0 b_0) are given for the level and slope parameters, the recursion can only start in time period 3, as start values will be calculated using the first two observations as l_2 = y_2 and b_2 = y_2 - y_1.  Given the input vector y has length N, the output vector f is still has the length (N+h), as well as the output matrix D still has N rows. The not available values for periods 1 and 2 are put as NaN[1] to avoid using a Union{T, Missing} type in julia, which would make a type recast necessary. ","category":"page"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"[1]: Be aware that Julia follows the IEEE 754 standard for floating-point values. The operation NaN == NaN will result in false, which makes e.g. comparisons  of vectors very tricky.","category":"page"},{"location":"man/exponential/#Optimization","page":"Exponential Smoothing","title":"Optimization","text":"","category":"section"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"There is no general solution to the problem of selecting the optimal parameter values for any of the above introduced models. The Optim.jl package provides box constrained algorithms to find the parameters that minimize the sum of squres error, this is equivalent to using a mean squared error function as loss function. But because of the complexity involved it is not guarannteed that a global minimum will be found, but rather a local minimum. ","category":"page"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"By using this procedure, the internal consistency, which is in favour of achieving the best fit for a decomposition, is choosen over external validity, which the preferred criterium when making forecasts.  But the best approach for extensive forecasting is to use a framework or julia package which generally supports cross-validation for evaluating any forecast model with a horizon h greater than 1. ","category":"page"},{"location":"man/exponential/#How-to-get-the-optimized-parameters","page":"Exponential Smoothing","title":"How to get the optimized parameters","text":"","category":"section"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"By using multiple dispatch the above introduced functions can basically be used without specifying any smoothing parameters for them to be estimated automatically. The current implementation only allows for all smoothing parameters to be omitted all together.  In addition, the damping parameter can also be estimated, which is the default, otherwise it has to be manually set e.g. varphi = 10 for it to have no impact.","category":"page"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"As an example, instead of setting the parameters manually like in holtWinters(data, 0.9, 0.9, 0.9, seasons), all smoothing parameters must be droped for the automatic optimization of the smoothing and dumping parameters and the function simplifies to holtWinters(data, seasons).","category":"page"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"A named tuple is returned and the estimated parameters can be accessed in two ways: ","category":"page"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"\tf, D, p = holtWinters(data, seasons, φ=1.0)","category":"page"},{"location":"man/exponential/","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"\tres = holtWinters(data, seasons, φ=1.0)\n\t\n\tf = res.forecast\n\tD = res.model\n\tp = res.parameters\n\t","category":"page"},{"location":"man/exponential/#Methods-for-automatic-parameter-optimization","page":"Exponential Smoothing","title":"Methods for automatic parameter optimization","text":"","category":"section"},{"location":"man/exponential/#TrendDecomposition.holtWinters-Tuple{Vector, Int64}","page":"Exponential Smoothing","title":"TrendDecomposition.holtWinters","text":"holtWinters(y :: Vector, s::Int;\n                 startValues::Tuple=(), h::Int = 0, model=:add, φ::Union{Real, Nothing} = nothing)\n\nHolt-Winters method with damping factor φ (\\varphi) given vector y (N x 1) and h forecast periods.\n\nThe number of seasons s has to be specified (s>0), else use TrendDecomposition.holtLinear. \n\nThe smoothing parameters are estimated by minimizing the loss function using Optim.jl. For φ equal nothing, its value is also determined by the optimization algorithm.\n\nStart values of level and slope can be given as tuple (l₀, b₀), otherwise output values start at period 3.\n\nReturns a tuple with (f, D, p), where f is a ((N+h) x 1) vector with the forecast values and D is a (N x 3) matrix with the level, slope, season values as columns, and p is a vector containing the estimated parameter(s).\n\n\n\n\n\n","category":"method"},{"location":"man/exponential/#TrendDecomposition.holtLinear-Tuple{Vector}","page":"Exponential Smoothing","title":"TrendDecomposition.holtLinear","text":"holtLinear(y :: Vector;\n                startValues::Tuple=(), h::Int = 0, φ::Union{Real, Nothing} = nothing)\n\nHolt's linear trend method with damping factor φ (\\varphi) given vector y (N x 1) and h forecast periods.\n\nThe smoothing parameters are estimated by minimizing the loss function using Optim.jl. For φ equal nothing, its value is also determined by the optimization algorithm.\n\nStart values of level and slope can be given as tuple (l₀, b₀), otherwise output values start at period 3.\n\nReturns a tuple with (f, D, p), where f is a ((N+h) x 1) vector with the forecast values and D is a (N x 2) matrix with the level and slope as columns, and p is a vector containing the estimated parameter(s).\n\n\n\n\n\n","category":"method"},{"location":"man/exponential/#TrendDecomposition.brownLinear-Tuple{Vector}","page":"Exponential Smoothing","title":"TrendDecomposition.brownLinear","text":"brownLinear(y :: Vector; h::Int = 0, startValues::Tuple=(), φ::Union{Real, Nothing} = nothing)\n\nDouble exponential smoothing with damping factor φ (\\varphi) given vector y (N x 1) and h forecast periods. The smoothing parameter λ is estimated by minimizing the loss function using Optim.jl. For φ equal nothing, its value is also determined by the optimization algorithm.\n\nStart values of level and slope can be given as tuple (l₀, b₀), otherwise output values start at period 3.\n\nReturns a tuple with (f, D, p), where f is a (N+h) x 1 vector with the forecast values and D is a (N x 2) matrix with the level and slope as columns, and p is a vector containing the estimated parameter(s)\n\n\n\n\n\n","category":"method"},{"location":"man/moving/#Moving-Average-(MA)","page":"Moving Average","title":"Moving Average (MA)","text":"","category":"section"},{"location":"man/moving/#Rolling-Average","page":"Moving Average","title":"Rolling Average","text":"","category":"section"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"The name rolling average instead of moving average is choosen here, because by default the functions works like a rolling window that slides through the  entire time series from t = 1T, calculating a value for each datum and  it is up to the user to deceide to have the boundary values discarded. ","category":"page"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"For centered MA the choosen order p has to be odd and thus can be written as p = 2k + 1 so that","category":"page"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"\tMA_t(p) = frac1p sum_i=-k^k x_t+i","category":"page"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"With centered=false the last (p-1) lagging values are used instead, for this option the order p can be either even or odd. ","category":"page"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"\tMA_t(p) = frac1p sum_i=0^p-1 x_t-i","category":"page"},{"location":"man/moving/#TrendDecomposition.rollingAverage-Tuple{Vector, Int64}","page":"Moving Average","title":"TrendDecomposition.rollingAverage","text":"rollingAverage(y :: Vector, p :: Int; centered::Bool = true, discard::Bool = false, offset::Int = 0)\n\nComputes the moving average of the vector y with p data points.\n\nWith centered equal true, p has to be an odd number so that an equal number of leads and lags can be used. By default when centered = false, the computation includes the datum plus its most recent p-1 lagged values. This behavior can be changed by including an offset; a positive offset includes one additonal lead at the expense of the oldest lag value\n\nFor discard=false the function rolls over all data, even if not all p data points can be used.\n\n\n\n\n\n","category":"method"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"Both methods allow an offset for calculating the MA. With a positive offset an additional lead value is included at the expense of a lagged value and the reverse holds for negative offsets. Negativ offset are only allowed for  centered MA, since for centered=false the maximum amount of lagged values are already used by default.","category":"page"},{"location":"man/moving/#MA-with-weights","page":"Moving Average","title":"MA with weights","text":"","category":"section"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"One can also specify weights omega_i for each i-th term, subject to that they sum to unity, e.g. for centered MA","category":"page"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"\tWMA_t(p) = frac1p sum_i=-k^k omega_i x_t+i qquad  st sum_i=-k^k omega_i = 1","category":"page"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"For a MA of order p, the weights provided by the user have to be a vector of size (p x 1).","category":"page"},{"location":"man/moving/#TrendDecomposition.rollingAverage-Tuple{Vector, Int64, Vector}","page":"Moving Average","title":"TrendDecomposition.rollingAverage","text":"rollingAverage(y :: Vector, p :: Int, weights :: Vector;\n    centered::Bool = true, discard::Bool = false, offset::Int = 0)\n\nComputes the moving average of the vector y with p data points weighted by vector w.\n\nWith centered equal true, p has to be an odd number so that an equal number of leads and lags can be used. By default when centered = false, the computation includes the datum plus its most recent p-1 lagged values. This behavior can be changed by including an offset; a positive offset includes one additonal lead at the expense of the oldest lag value.\n\nFor discard=false the function rolls over all data, even if not all p data points can be used, in any case all used weights will sum to 1.\n\n\n\n\n\n","category":"method"},{"location":"man/moving/#Calculation-for-boundary-or-edge-values","page":"Moving Average","title":"Calculation for boundary or edge values","text":"","category":"section"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"When using a centered moving average of order p, for the first fracp-12 and the last fracp-12 values  of the time series there are not enought data points available. To generalise about these boundary cases, the normal centered MA can also be thought as a special case of weighted MA with uniform weights, where each weight must equal  w_i = frac1p. For k missing values, the weights that still can be applied are normalized so that they sum to 1, so that each of the (p-k) remaining weights now equals frac1p-k.  As an example, the calculation of the first value of a time series y using a centered MA of order 5 becomes  MA_1(5) = sum_i=1^3 frac13 y_i = frac13 sum_i=1^3 y_i.","category":"page"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"For weighted MA with given weights w_1w_p, in case that for the first k weights there are no data points available for the  calculation, the remaining weights are normalized as follows w_i = fracw_isum_j=k+1^pw_j. Correspondingly, in case the last k weights cannot be used, the remaining p-k weights are normalized: w_i = fracw_isum_j=1^p-kw_j","category":"page"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"As it is common to drop data for which the full order of the specified MA cannot be computed,  discard=true will return NaN[1] for each datum in the output vector, where not enough data points are available due to boundary cases.","category":"page"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"[1]: Be aware that Julia follows the IEEE 754 standard for floating-point values. The operation NaN == NaN will result in false, which makes e.g. comparisons  of vectors very tricky.","category":"page"},{"location":"man/moving/#Seasonal-Average","page":"Moving Average","title":"Seasonal Average","text":"","category":"section"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"For each season component S_i this function calculates the average for all observation that where recorded in the same type of season given 12s types.","category":"page"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"Since the formal mathematical notation requires the use of sets, the interested reader can see the formula for calculating the seasonal averages in the notes down below[2].","category":"page"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"[2]: For the time series y_1 y_2y_T of length T and the number of seasons s, each observation y_t t in 1T,  can belong only to one set mathcalS_i, i in 1s out of all possible s sets, where each set is comprised of all observations that take place in the same type of season.  Therefore the superset mathcalS, consisting of all individual disjoint sets mathcalS_i, can be defined as  mathcalS = bigcup_i = 1^s mathcalS_iwhere  mathcalS_i = y_t  ((t-1) mod s) + 1 = i and mathcalS_i cap mathcalS_j = emptyset for all i neq j. So the seasonal average for the i-th number of season S_i can be written and computed asS_i = frac1mathcalS_i sum_x in mathcalS_i x\n","category":"page"},{"location":"man/moving/#TrendDecomposition.maSeason","page":"Moving Average","title":"TrendDecomposition.maSeason","text":"maSeason(y :: Vector, seasons :: Int; repeating::Bool = false)\n\nGiven the number of seasons, the function computes the average value of each season component. This method works better for detrended data.\n\nWith repeating equal true the function will repeat the results until the output vector has the same length as the input vector y.\n\n\n\n\n\n","category":"function"},{"location":"man/moving/#Classical-decomposition","page":"Moving Average","title":"Classical decomposition","text":"","category":"section"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"Given the implementations of maSeason and rollingAverage, the following two decompositions using moving averages can be archived: ","category":"page"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"The additive model","category":"page"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"\ty_t = tau_t + S_t + u_t","category":"page"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"or the multiplicative model","category":"page"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"\ty_t = tau_t * S_t * u_t","category":"page"},{"location":"man/moving/","page":"Moving Average","title":"Moving Average","text":"where tau_t is the trend component estimated using moving averages, S_t is the average season component estimated using the detrended time series and u_t is simply the residual factor defined as u_t = y_t - tau_t - S_t.","category":"page"},{"location":"man/moving/#TrendDecomposition.maDecompose","page":"Moving Average","title":"TrendDecomposition.maDecompose","text":"maDecompose(y :: Vector, seasons :: Int; combine::Bool = false, model=:add)\n\nDecomposes the time series y into trend (T), season (S) and noise components (I).\n\nEither assumes an additive model (:add) or a multiplicative model (:mul).\n\nReplicates the R decompose{stats} function. For a more generic function implementation see decompose of this package (TrendDecomposition.decompose).\n\nReturns a matrix where the columns correspont to the above mentioned components in (T, S, I) order; with combine equal true it returns (y, T, S, I) as columns.\n\n\n\n\n\n","category":"function"},{"location":"#TrendDecomposition.jl","page":"Introduction","title":"TrendDecomposition.jl","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Welcome to the TrendDecomposition.jl documentation.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"info: Info\nThis is a preliminary version of the documentation. The package is also not feature complete until version 1.0, thus sometimes there are references to not yet implemented features.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"TrendDecomposition.jl is a Julia package for decomposition of time series into trend and cycle components. More generally it provides  both (stochastic) trend component estimation and forecasting, though not all methods are suitable for forecasting.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"By using filters and smoothers the most pragmatic approach to trend decomposition is estimating the trend t and defining the cyclical component c of time series y as c = y - t. Often it is up to the user of this module to calculate the cyclical components themselves with the computed trend returned from a function  provided by this module.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"The following is list of already implemented and documented methods:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Exponential Smoothing\nSimple exponential smoothing\nDouble exponential smoothing / Brown linear method\nHolt Linear procedure\nHolt Winters method\nPenalized smoothing\nBohlmann Filter / Whittaker-Henderson Smoothing\nLeser / Hodrick-Prescott (HP) Filter\nBoosted HP Filter\nMoving Average (MA)\nSeasonal Average\nClassical Decomposition by moving averages","category":"page"},{"location":"man/api/#Functions","page":"API","title":"Functions","text":"","category":"section"},{"location":"man/api/","page":"API","title":"API","text":"","category":"page"}]
}
