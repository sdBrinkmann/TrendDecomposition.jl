var documenterSearchIndex = {"docs":
[{"location":"man/penalized/#Penalized-Smoothing","page":"Penalized Smoothing","title":"Penalized Smoothing","text":"The term penalized smoothing here denotes a class of smoothers  that minimize a criterion  subject to a penalizing term. The methods introduced here are in fact one of the earliest uses in statistics of a  penalizing term first introduced in 1898 by George Bohlmann[Bohl99] and it is the first smoothing procedure posed as an optimization problem. \n\n[Bohl99]: Bohlmann, G. (1899). Ein Ausgleichungsproblem. Nachrichten von der Gesellschaft der Wissenschaften zu Göttingen, Mathematisch-Physikalische Klasse, 1899, 260-271.\n\nGiven our time series y, it is assumed that it can be decomposed into\n\n\ty_t = tau_t + c_t\n\nwhere tau_t is the trend component and c_t is the cycle component at time t.  The criterion we want to minimize is the squared difference between the  trend component and the corresponding data point (y_t - tau_t)^2.\n\nFor the penalizing term we differentiate between the traditionally used l_2 norm and more novel approaches using the l_1 norm, similar to L1 and L2 regularization in regression analysis. ","category":"section"},{"location":"man/penalized/#Whittaker-Henderson-Smoothing","page":"Penalized Smoothing","title":"Whittaker-Henderson Smoothing","text":"Using the squared m'th difference of tau_t, written here as (Delta^m tau_t)^2, as penalizing term and lambda  0 as regularization parameter, the trend component can be estimated as solution to the optimization problem:\n\n^tau_t = \toperatorname*argmin_tau Bigg sum_t=1^n (y_t - tau_t)^2 + lambda sum_t=m+1^n \n(Delta^m tau_t)^2Bigg\n\nAll functions below, bohlmannFilter, hpFilter, bhpFilter, use the algebraic solution to the minimization problem.  The lambda value has to be selected beforehand by the user. For lambda to 0 the solution converges to the original data and for lambda to infty the trend becomes a straight line. ","category":"section"},{"location":"man/penalized/#Hodrick-Prescott-(HP)-Filter","page":"Penalized Smoothing","title":"Hodrick-Prescott (HP) Filter","text":"The case using second difference with m = 2 was proposed by Leser (1961)[Leser61] for trend estimation and the above equation becomes \n\n[Leser61]: Leser, C. E. V. (1961). A Simple Method of Trend Construction. Journal of the Royal Statistical Society. Series B (Methodological), 23(1), 91–107.\n\n^tau_t = \toperatorname*argmin_tau Bigg sum_t=1^n (y_t - tau_t)^2 + lambda sum_t=m+1^n \n(tau_t+1 - 2 tau_t + tau_t-1)^2Bigg\n\nIn the literature this procedure is also known as the Hodrick-Prescott filter.  In general, the following parameters are recommended for lambda\n\nBasis Period length λ\nAnnual 1 100\nQuarterly 4 1600\nMonthly 12 14,600","category":"section"},{"location":"man/penalized/#Boosted-HP-Filter","page":"Penalized Smoothing","title":"Boosted HP Filter","text":"In order to provide a better trend estimation, according to Philips and Shi (2021)[PhilShi21] the HP Filter can be applied repeatedly over the cycle estimate(s). This idea is akin to L_2-boosting in machine learning, hence the method is called boosted HP (bHP) filter. \n\n[PhilShi21]: Phillips, P.C.B. and Shi, Z. (2021), BOOSTING: WHY YOU CAN USE THE HP FILTER. International Economic Review, 62: 521-570.\n\nhpFilter(x::Vector, λ::Real, iter::Int) allows the repeated application of the HP filter by a fixed amount of times determind by the parameter iter.\n\nbhpFilter implements an iterative approach, where a stopping criterion determines the amout of iterations.","category":"section"},{"location":"man/penalized/#l_1-Trend-Filtering","page":"Penalized Smoothing","title":"l_1 Trend Filtering","text":"","category":"section"},{"location":"man/penalized/#Taut-String-Piecewise-constant","page":"Penalized Smoothing","title":"Taut String - Piecewise constant","text":"The taut string algorithm by Davies and Kovac (2001)[DaKo01] can be seen as an efficient O(n) solution to the following minimization problem:\n\n[DaKo01]: Davies, P. L., & Kovac, A. (2001). Local extremes, runs, strings and multiresolution. The Annals of Statistics, 29(1), 1-65\n\n^tau_t = \toperatorname*argmin_tau Bigg sum_t=1^n (y_t - tau_t)^2 + lambda sum_t=m+1^n tau_t - tau_t-1Bigg\n\nwhere tau is again the trend component. The result is a piecewise constant function and it can be shown that is uses the smallest number of knots.\n\nAlternatively, using the term total variation (TV) of a function f the taut string approach minimizes following function:\n\n sum_t=1^n (y_t - f(x_t))^2 + lambda TV(f)\n\nalso known as total variation regularization or denoising.\n\nInstead of the parameter lambda, the parameter C is used to determine the radius of the tube in the taut string method, but in practise C is equivalent to the parameter lambda in the alternative estimation methods below (ADMM and fused Lasso).\n\nThis function implements a suggestion by the authors of the algorithm to vary the starting and end points in order to find the best fit. With optimize = true various end point are tested and the best outcome is choosen. This is a pure experimental feature. Run julia with julia --threads 5 to take advantage of multi-threading. ","category":"section"},{"location":"man/penalized/#Alternative-Direction-Method-of-Multipliers-(ADMM)","page":"Penalized Smoothing","title":"Alternative Direction Method of Multipliers (ADMM)","text":"To use ADMM as a solution method to total variation minimization was proposed by Boyd et al. (2011)[Boyd11]\n\n[Boyd11]: Boyd, S., Neal, P., Eric, C., Borja, P., & Jonathan, E. (2011). Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends® in Machine learning, 3(1), 1-122.\n\nand is here implemented as a general solver for any order of l1 trend filtering. ","category":"section"},{"location":"man/penalized/#Alternatives-to-Soft-Thresholding","page":"Penalized Smoothing","title":"Alternatives to Soft-Thresholding","text":"Ramas and Tibshirani (2016)[RaTi12] proposed using the fused Lasso instead of soft-thresholding to archieve faster convergence and a more numerical robust solution. A second alternative is offered here using the taut string algorithm instead of the fused Lasso.\n\n[RaTi12]: Ramdas, A., & Tibshirani, R. J. (2016). Fast and flexible ADMM algorithms for trend filtering. Journal of Computational and Graphical Statistics, 25(3), 839-858","category":"section"},{"location":"man/penalized/#Taut-String","page":"Penalized Smoothing","title":"Taut String","text":"In the same vein as with tautStringFit above, here opt=true can be used to experiment with various  end points in the taut string algorithm.","category":"section"},{"location":"man/penalized/#Fused-Lasso","page":"Penalized Smoothing","title":"Fused Lasso","text":"This function is offered as an extension for the package Lasso.jl. Please use with using Lasso.","category":"section"},{"location":"man/penalized/#Note-on-augmented-Lagrangian-parameter-ρ","page":"Penalized Smoothing","title":"Note on augmented Lagrangian parameter ρ","text":"Using ADMM with soft-thresholding or taut string the augmented Lagrangian parameter rho (written as \\rho<TAB>) can be tuned to archieve a faster convergence in practise. As a parameter for the subroutine using either soft-thresholding or taut string lambda  rho is used with rho = lambda as default (same default for fused Lasso). Using a rho greater than or rather a multiple of lambda can increase the convergence rate significantly. ","category":"section"},{"location":"man/penalized/#TrendDecomposition.bohlmannFilter","page":"Penalized Smoothing","title":"TrendDecomposition.bohlmannFilter","text":"bohlmannFilter(x :: Vector, m :: Int, λ :: Real)\n\nThis is the generalization of the Hodrick-Prescott filter, also known as Whittaker-Henderson smoothing, using the m-th difference to estimate the trend component.\n\n\n\n\n\n","category":"function"},{"location":"man/penalized/#TrendDecomposition.hpFilter-Tuple{Vector, Real}","page":"Penalized Smoothing","title":"TrendDecomposition.hpFilter","text":"hpFilter(x::Vector, λ::Real)\n\nApply the Hodrick-Prescott decomposition to vector x with multiplier value λ.\n\nFunction returns the trend component.\n\n\n\n\n\n","category":"method"},{"location":"man/penalized/#TrendDecomposition.hpFilter-Tuple{Vector, Real, Int64}","page":"Penalized Smoothing","title":"TrendDecomposition.hpFilter","text":"hpFilter(x::Vector, λ::Real, iter::Int)\n\nCompute boosted Hodrick-Prescott filter with number of iterations specified by iter.\n\nFunction returns the trend component.\n\n\n\n\n\n","category":"method"},{"location":"man/penalized/#TrendDecomposition.bhpFilter","page":"Penalized Smoothing","title":"TrendDecomposition.bhpFilter","text":"bhpFilter(x::Vector, λ::Real; Criterion=\"BIC\", max_iter::Int = 100, p::Float64=0.05)\n\nComputes the boosted Hodrick-Prescott filter by appyling the filter iteratively over the trend component with stop criterion being either a Bayesian-type information criterion (BIC) or an augmented Dickey-Fuller (ADF) test.\n\nFunction returns the trend component.\n\n\n\n\n\n","category":"function"},{"location":"man/penalized/#TrendDecomposition.tautStringFit","page":"Penalized Smoothing","title":"TrendDecomposition.tautStringFit","text":"tautStringFit(y :: Vector, C :: Real; optimize::Bool=false)\n\nComputes the taut string for time series y where C determines the radius of the tube.\n\nReturns (string, (x, y)) with string being the fitted taut string and the x and y coordinates of the knots.\n\n\n\n\n\n","category":"function"},{"location":"man/penalized/#TrendDecomposition.trendADMM","page":"Penalized Smoothing","title":"TrendDecomposition.trendADMM","text":"trendADMM(y :: Vector, λ :: Real; m::Int = 2, max_iter::Int = 2000, ρ::Real=λ,\n            ϵ_abs::Float64=1.e-4, ϵ_rel::Float64=1.e-3)\n\nTrend filtering of time series data y by using the L1 penalty with regularization parameter λ and using the m'th difference. The estimation procedure uses the Alternating Direction Method of Multipliers (ADMM) to reach a numerical solution.\n\nThe function returns the estimated trend component.\n\n\n\n\n\n","category":"function"},{"location":"man/penalized/#TrendDecomposition.tautADMM","page":"Penalized Smoothing","title":"TrendDecomposition.tautADMM","text":"tautADMM(y :: Vector, λ :: Real;\n            m::Int = 2, max_iter::Int = 2000, ρ::Real=λ, opt::Bool = false,\n            ϵ_abs::Float64=1.e-4, ϵ_rel::Float64=1.e-2)\n\nTrend filtering of time series data y by using the L1 penalty with regularization parameter λ and using the m'th difference. The estimation procedure uses the Alternating Direction Method of Multipliers (ADMM) in combination with the taut string algorithm to reach a numerical solution. For m = 1 only the taut string algorithm is used as an edge case solution. \n\nThe function returns the estimated trend component.\n\n\n\n\n\n","category":"function"},{"location":"man/penalized/#TrendDecomposition.fusedADMM","page":"Penalized Smoothing","title":"TrendDecomposition.fusedADMM","text":"fusedADMM(y :: Vector, λ :: Real; m::Int = 2, max_iter::Int = 1000, ρ::Real=λ,\n            ϵ_abs::Float64=1.e-4, ϵ_rel::Float64=1.e-2)\n\nPlaceholder for FusedADMM extension, when using TrendDecomposition together with Lasso.jl package.\n\nTrend filtering of time series data y by using the L1 penalty with regularization parameter λ and using the m'th difference. The estimation procedure uses the Alternating Direction Method of Multipliers (ADMM) in combination with the fused lasso algorithm from the Lasso.jl package to reach a numerical solution. For m = 1 only the taut string algorithm is used as an edge case solution. \n\nThe function returns the estimated trend component.\n\n\n\n\n\n","category":"function"},{"location":"man/start/#How-to-add-TrendDecomposition-package","page":"Get Started","title":"How to add TrendDecomposition package","text":"This package is now featured on the official general Julia package registry.  Simply use Julia's package manager pkg to add TrendDecomposition to your preferred environment.\n\n@(v1.11) pkg> add TrendDecomposition\n\njulia> using TrendDecomposition\n\nThe developing branch of this package can either be employed  by cloning this repository or by using the Julia package manager. With the package manager simply use the add command:\n\n@(v1.11) pkg> add https://github.com/sdBrinkmann/TrendDecomposition.jl\n\nFor the developing branch one can alternatively try with add to fetch from the repository:\n\n@(v1.11) pkg> add https://github.com/sdBrinkmann/TrendDecomposition.jl\n\nwarning: Warning\nThis package is currently under development and follows Semantic Versioning. Until the 1.0.0 release is reached, the API of this package can change with any minor version update,  please  consult the documentation of this package after each update when using this package.","category":"section"},{"location":"man/start/#Usage","page":"Get Started","title":"Usage","text":"Below are serveral examples which demonstrate typical usecases of various functions contained in  TrendDecomposition.jl. ","category":"section"},{"location":"man/start/#Hodrick-Prescott-Filter","page":"Get Started","title":"Hodrick-Prescott Filter","text":"The basic usage of trend estimation using the Hodrick-Prescott filter is demonstrated with the US industrial production index (IPI) provided by FRED data service.\n\nusing TrendDecomposition\nusing CSV\n\n# Set path to directory where time series is located\npath = \"/.../data\"\n\nIPI = CSV.read(\"$(path)/IPB50001SQ.csv\", copycols=true)\n\n# HP filter with λ = 1600\nhp = hpFilter(IPI[!, 2], 1600)\n\n# The above is equivalent to Whittaker-Henderson smoothing with m = 2 differentiation\nwh = bohlmannFilter(IPI[!, 2], 2, 1600)\n\n# Boosted HP filter with baysian-type information criterion (BIC)\nbHP_bic = bhpFilter(IPI[!, 2], 1600, Criterion=\"BIC\")\n\n# Boosted HP filter with augmented Dickey-Fuller (ADF) test \nbHP_adf = bhpFilter(IPI[!, 2], 1600, Criterion=\"ADF\", p=0.01)\n\n(Image: HP Results)","category":"section"},{"location":"man/start/#Classical-decomposition","page":"Get Started","title":"Classical decomposition","text":"This package implements moving averages with rollingAverage and seasonal averages with maSeason. With the help of both functions, we can conduct time series decompositions into a trend, seasonal and noise component. The function maDecompose replicates the decompose{stats} functionality implemented in R.\n\nusing Plots\nusing TrendDecomposition\n\n# Kendall M. G., Stuard A. (1983). The Advanced Theory of Statistics, Vol. 3\nx = [-50, 175, 149, 214, 247, 237, 225, 329, 729, 809,\n       530, 489, 540, 457, 195, 176, 337, 239, 128, 102, 232, 429, 3,\n     98, 43, -141, -77, -13, 125, 361, -45, 184]\n\n\nres = maDecompose(x, 4, combine=true)\n\nplot(res; layout = (4, 1), title=[\"y\" \"trend\" \"season\" \"noise\"], legend=false, tickfontsize=4)\n\n\n(Image: Classical Decomposition)","category":"section"},{"location":"man/start/#Holt-Winters-Method","page":"Get Started","title":"Holt-Winters Method","text":"For both local trend and seasonal component estimation, the Holt-Winters method can be used. The two following examples illustrate its dual usecase of modeling a trend and seasonal component for both decomposition and for forecasting time series.","category":"section"},{"location":"man/start/#Decomposition","page":"Get Started","title":"Decomposition","text":"using Plots\nusing CSV\nusing DataFrames\n\nusing TrendDecomposition\n\n# Box, G.E.P., Jenkins, G.M. and Reinsel, G.C. (1994) Time Series Analysis; Forecasting and Control. 3rd Edition\nair = CSV.read(\"./AirPassengers.csv\", DataFrame, copycols=true)\ndata = air[!, 2]\n\nseasons = 12\n\n# Decomposition using multiplicative model with automatic optimization\nf1, D1, p1 = holtWinters(data, seasons, model=:mul)\n\n# difference between data points and estimated values\nresidual = data .- f1\n\nplot([data D1 residual], layout = (5, 1), ylabel = [\"y\" \"level\" \"slope\" \"season\" \"residual\"], legend=false, tickfontsize=4, color=:black, guidefontsize=8)\n\n\n(Image: HW Decomposition)","category":"section"},{"location":"man/start/#Forecasting","page":"Get Started","title":"Forecasting","text":"The damping factor varphi can be typed in julia with the keyboard as follows: \\varphi<tab>\n\n\n# Forecast horizon h = 24 and damping factor \\varphi = 0.95\nf2, D2, p2 = holtWinters(data, seasons, h=24, model=:mul, φ = 0.95)\n\nn = length(data)\nsteps = (n+1):(n+24)\n\nplot(data, color=:black, legend=false, title=\"HW forecast h=24\")\nplot!(steps, f2[(end-23):end], color=:red, label=\"forecast\")\n\n(Image: HW forecasting)","category":"section"},{"location":"man/start/#Taut-String","page":"Get Started","title":"Taut String","text":"Taut string is a efficient O(n) algorithm for total variation regularization.\n\n# Data from: Donoho, D. L. et al (1995) Wavelet Shrinkage: Asymptopia? Journal of the Royal Statistical Society series B, 57, 301–337. \n\nstring, (x, y) = tautStringFit(djblocks, 10)\n\n# (x, y) are coordiantes of knots\n\nplot(djblocks, label=\"y\")\nplot!(string, label=\"string\")\n\n\n(Image: Taut String)","category":"section"},{"location":"man/exponential/#Exponential-Smoothing","page":"Exponential Smoothing","title":"Exponential Smoothing","text":"Exponential smoothing, also known as exponentially weighted moving average (EWMA), allows us to model a time series as an additive or multiplicative composition, in order to conduct forecasts. Here in its additive form it can be written as \n\n\ty_t = l_t + S_t + u_t\n\nwhere l denotes the level or trend component and S the seaonal component, the  latter component is only included in the Holt-Winters method.[Winters60] \n\n[Winters60]: Winters, P.R. (1960). Forecasting sales by exponentially weighted moving averages. Management Science 6: 324-42\n\nThe residual or noise term u_t is implicitly assumed and the parameters of the  models down below are estimated by minimizing the squared residual terms, ^u_t = y_t - ^y_t,  summed over the entire time series. The model makes otherwise no assumptions and is stated in its classic recursive fashion.","category":"section"},{"location":"man/exponential/#Simple-exponential-smoothing","page":"Exponential Smoothing","title":"Simple exponential smoothing","text":"Simple exponential smoothing is a special case of weighted moving averages, where weights decline exponentially. Implemented as a recursion it is know as  exponentially weighted moving average (EWMA) defined as:\n\n    l_t = (1-lambda) * l_t-1 + lambda * y_t\n\nFor the recursion to work a start value has to be defined. Given that the time series y_t is recorded for t = 1T, a start value y_0 has to be selected.","category":"section"},{"location":"man/exponential/#Holt's-procedure","page":"Exponential Smoothing","title":"Holt's procedure","text":"In order to account for a trend and to make forecasts a local slope b_t was introduced by Holt (1957)[Holt57] , which is updated each period t.  As for most time series it is unrealistic to assume that a linear line is to presist much long into the future, a damping factor varphi (\\varphi) can be used and is set between 0  varphi  1.\n\n[Holt57]: Holt, Charles C. (1957). \"Forecasting Trends and Seasonal by Exponentially Weighted Averages\". Office of Naval Research Memorandum\n\n\tbeginaligned\n    l_t = lambda_1 * y_t + (1-lambda_1) * (l_t-1 + varphi b_t-1) \n\tb_t = lambda_2 * (l_t + l_t-1) + (1-lambda_2) * varphi b_t-1 \n\t^y_t = l_t-1 + varphi b_t -1\n\tendaligned","category":"section"},{"location":"man/exponential/#Double-exponential-smoothing","page":"Exponential Smoothing","title":"Double exponential smoothing","text":"By using the above introduced residual or error term ^u_t, the level l_t and slope b_t parameters can be estimated as:\n\n\tbeginaligned\n\tl_t = l_t-1 + varphi b_t-1 + (1 - lambda^2) ^u_t \n\tb_t = varphi b_t-1 + (1 - lambda)^2 ^u_t  \n\t^y_t = l_t-1 + varphi b_t -1\n\tendaligned\n\nThis recursion is known as double exponential smoothing and also named after R.G. Brown (1963)[Brown63] and can been seen as a special case of Holt's procedure by setting lambda_0 = 1 - lambda^2 and lambda_1 = frac1 - lambda1 + lambda.\n\n[Brown63]: Brown, R.G. (1963). Smoothing, Forecasting and Prediction. Englewood Cliffs: Prentice Hall.","category":"section"},{"location":"man/exponential/#Holt-Winters-forecasting","page":"Exponential Smoothing","title":"Holt-Winters forecasting","text":"In addition a seasonal component S_t can be modeled for number of seasons s. This introduces an additional equation for S_t: \n\n\tbeginaligned\n    l_t = lambda_1 * (y_t - S_t-s) + (1-lambda_1) * (l_t-1 + varphi b_t-1) \n\tb_t = lambda_2 * (l_t + l_t-1) + (1-lambda_2) * b_t-1 varphi \n    S_t = lambda_3 * (y_t + l_t) + (1-lambda_3) * S_t-s \n\t^y_T+h = l_T + b_T (varphi + varphi^2 ++ varphi^h) + S_T+h-s\n\tendaligned\n\nAs for the t=1,...,s first values there exists no previous values for S_t,  they have to be estimated before starting the recursion computation.  maSeason will be used for the first 2*s values of the detrended time series.  Similar for forecast horizions h > s, the same latest s estimated season components from time periods (T-s-1) to T have to be used over again.","category":"section"},{"location":"man/exponential/#General-behavior-of-holtLinear,-brownLinear-and-holtWinters","page":"Exponential Smoothing","title":"General behavior of holtLinear, brownLinear and holtWinters","text":"By default varphi=1 in holtLinear, brownLinear and holtWinters method implementations, so that no damping takes place unless explicitly set. If no start values (l_0 b_0) are given for the level and slope parameters, the recursion can only start in time period 3, as start values will be calculated using the first two observations as l_2 = y_2 and b_2 = y_2 - y_1.  Given the input vector y has length N, the output vector f is still has the length (N+h), as well as the output matrix D still has N rows. The not available values for periods 1 and 2 are put as NaN[1] to avoid using a Union{T, Missing} type in julia, which would make a type recast necessary. \n\n[1]: Be aware that Julia follows the IEEE 754 standard for floating-point values. The operation NaN == NaN will result in false, which makes e.g. comparisons  of vectors very tricky.","category":"section"},{"location":"man/exponential/#Optimization","page":"Exponential Smoothing","title":"Optimization","text":"There is no general solution to the problem of selecting the optimal parameter values for any of the above introduced models. The Optim.jl package provides box constrained algorithms to find the parameters that minimize the sum of squres error, this is equivalent to using a mean squared error function as loss function. But because of the complexity involved it is not guarannteed that a global minimum will be found, but rather a local minimum. \n\nBy using this procedure, the internal consistency, which is in favour of achieving the best fit for a decomposition, is choosen over external validity, which the preferred criterium when making forecasts.  But the best approach for extensive forecasting is to use a framework or julia package which generally supports cross-validation for evaluating any forecast model with a horizon h greater than 1. ","category":"section"},{"location":"man/exponential/#How-to-get-the-optimized-parameters","page":"Exponential Smoothing","title":"How to get the optimized parameters","text":"By using multiple dispatch the above introduced functions can basically be used without specifying any smoothing parameters for them to be estimated automatically. The current implementation only allows for all smoothing parameters to be omitted altogether.  In addition, the damping parameter can also be estimated, which is the default, otherwise it has to be manually set e.g. varphi = 10 for it to have no impact.\n\nAs an example, instead of setting the parameters manually like in holtWinters(data, 0.9, 0.9, 0.9, seasons), all smoothing parameters must be droped for the automatic optimization of the smoothing and dumping parameters and the function simplifies to holtWinters(data, seasons).\n\nA named tuple is returned and the estimated parameters can be accessed in two ways: \n\n\tf, D, p = holtWinters(data, seasons, φ=1.0)\n\n\tres = holtWinters(data, seasons, φ=1.0)\n\t\n\tf = res.forecast\n\tD = res.model\n\tp = res.parameters\n\t","category":"section"},{"location":"man/exponential/#Methods-for-automatic-parameter-optimization","page":"Exponential Smoothing","title":"Methods for automatic parameter optimization","text":"","category":"section"},{"location":"man/exponential/#TrendDecomposition.expSmoothing","page":"Exponential Smoothing","title":"TrendDecomposition.expSmoothing","text":"expSmoothing(y :: Vector, λ :: Real; startValue::Bool = true, v_0::Real = 0.0)\n\nSimple exponential smoothing with smoothing factor λ. If startValue equals true, v_0 will be used as initial value, otherwise the first element of y (y[1]) will be selected as starting value. Using y[1] as starting value will result in a different mathematical function.   \n\n\n\n\n\n","category":"function"},{"location":"man/exponential/#TrendDecomposition.holtLinear-Tuple{Vector, Real, Real}","page":"Exponential Smoothing","title":"TrendDecomposition.holtLinear","text":"holtLinear(y :: Vector, λ₁ :: Real, λ₂ :: Real;\n                startValues::Tuple=(), h::Int = 0, φ::Real = 1.0)\n\nHolt's linear trend method with smoothing factors λ₁ (level) and λ₂ (slope) and damping factor φ (\\varphi) given vector y (N x 1) and h forecast periods.\n\nStart values of level and slope can be given as tuple (l₀, b₀), otherwise output values start at period 3.\n\nReturns a tuple with (f, D), where f is a ((N+h) x 1) vector with the forecast values and D is a (N x 2) matrix with the level and slope as columns.\n\n\n\n\n\n","category":"method"},{"location":"man/exponential/#TrendDecomposition.brownLinear-Tuple{Vector, Real}","page":"Exponential Smoothing","title":"TrendDecomposition.brownLinear","text":"brownLinear(y :: Vector, λ :: Real; h::Int = 0, startValues::Tuple=(), φ::Real = 1.0)\n\nDouble exponential smoothing with smoothing factor λ and damping factor φ (\\varphi) given vector y (N x 1) and h forecast periods.\n\nStart values of level and slope can be given as tuple (l₀, b₀), otherwise output values start at period 3.\n\nReturns a tuple with (f, D), where f is a (N+h) x 1 vector with the forecast values and D is a (N x 2) matrix with the level and slope as columns.\n\n\n\n\n\n","category":"method"},{"location":"man/exponential/#TrendDecomposition.holtWinters-Tuple{Vector, Real, Real, Real, Int64}","page":"Exponential Smoothing","title":"TrendDecomposition.holtWinters","text":"holtWinters(y :: Vector, λ₁ :: Real, λ₂ :: Real, λ₃ :: Real, s::Int;\n                 startValues::Tuple=(), h::Int = 0, model=:add, φ::Real = 1.0)\n\nHolt-Winters method with smoothing factors λ₁ (level), λ₂ (slope) and λ₃ (season) and damping factor φ (\\varphi) given vector y (N x 1) and h forecast periods.\n\nThe number of seasons s has to be specified (s>0), else use TrendDecomposition.holtLinear. \n\nStart values of level and slope can be given as tuple (l₀, b₀), otherwise output values start at period 3.\n\nReturns a tuple with (f, D), where f is a ((N+h) x 1) vector with the forecast values and D is a (N x 3) matrix with the level, slope, season values as columns.\n\n\n\n\n\n","category":"method"},{"location":"man/exponential/#TrendDecomposition.holtWinters-Tuple{Vector, Int64}","page":"Exponential Smoothing","title":"TrendDecomposition.holtWinters","text":"holtWinters(y :: Vector, s::Int;\n                 startValues::Tuple=(), h::Int = 0, model=:add, φ::Union{Real, Nothing} = nothing)\n\nHolt-Winters method with damping factor φ (\\varphi) given vector y (N x 1) and h forecast periods.\n\nThe number of seasons s has to be specified (s>0), else use TrendDecomposition.holtLinear. \n\nThe smoothing parameters are estimated by minimizing the loss function using Optim.jl. For φ equal nothing, its value is also determined by the optimization algorithm.\n\nStart values of level and slope can be given as tuple (l₀, b₀), otherwise output values start at period 3.\n\nReturns a tuple with (f, D, p), where f is a ((N+h) x 1) vector with the forecast values and D is a (N x 3) matrix with the level, slope, season values as columns, and p is a vector containing the estimated parameter(s).\n\n\n\n\n\n","category":"method"},{"location":"man/exponential/#TrendDecomposition.holtLinear-Tuple{Vector}","page":"Exponential Smoothing","title":"TrendDecomposition.holtLinear","text":"holtLinear(y :: Vector;\n                startValues::Tuple=(), h::Int = 0, φ::Union{Real, Nothing} = nothing)\n\nHolt's linear trend method with damping factor φ (\\varphi) given vector y (N x 1) and h forecast periods.\n\nThe smoothing parameters are estimated by minimizing the loss function using Optim.jl. For φ equal nothing, its value is also determined by the optimization algorithm.\n\nStart values of level and slope can be given as tuple (l₀, b₀), otherwise output values start at period 3.\n\nReturns a tuple with (f, D, p), where f is a ((N+h) x 1) vector with the forecast values and D is a (N x 2) matrix with the level and slope as columns, and p is a vector containing the estimated parameter(s).\n\n\n\n\n\n","category":"method"},{"location":"man/exponential/#TrendDecomposition.brownLinear-Tuple{Vector}","page":"Exponential Smoothing","title":"TrendDecomposition.brownLinear","text":"brownLinear(y :: Vector; h::Int = 0, startValues::Tuple=(), φ::Union{Real, Nothing} = nothing)\n\nDouble exponential smoothing with damping factor φ (\\varphi) given vector y (N x 1) and h forecast periods. The smoothing parameter λ is estimated by minimizing the loss function using Optim.jl. For φ equal nothing, its value is also determined by the optimization algorithm.\n\nStart values of level and slope can be given as tuple (l₀, b₀), otherwise output values start at period 3.\n\nReturns a tuple with (f, D, p), where f is a (N+h) x 1 vector with the forecast values and D is a (N x 2) matrix with the level and slope as columns, and p is a vector containing the estimated parameter(s)\n\n\n\n\n\n","category":"method"},{"location":"man/moving/#Moving-Average-(MA)","page":"Moving Average","title":"Moving Average (MA)","text":"","category":"section"},{"location":"man/moving/#Rolling-Average","page":"Moving Average","title":"Rolling Average","text":"The name rolling average instead of moving average is choosen here, because by default the functions works like a rolling window that slides through the  entire time series from t = 1T, calculating a value for each datum and  it is up to the user to decide to have the boundary values discarded. \n\nFor centered MA the choosen order p has to be odd and thus can be written as p = 2k + 1 so that\n\n\tMA_t(p) = frac1p sum_i=-k^k x_t+i\n\nWith centered=false the last (p-1) lagging values are used instead, for this option the order p can be either even or odd. \n\n\tMA_t(p) = frac1p sum_i=0^p-1 x_t-i\n\nBoth methods allow an offset for calculating the MA. With a positive offset an additional lead value is included at the expense of a lagged value and the reverse holds for negative offsets. Negativ offset are only allowed for  centered MA, since for centered=false the maximum amount of lagged values are already used by default.","category":"section"},{"location":"man/moving/#MA-with-weights","page":"Moving Average","title":"MA with weights","text":"One can also specify weights omega_i for each i-th term, subject to that they sum to unity, e.g. for centered MA\n\n\tWMA_t(p) = frac1p sum_i=-k^k omega_i x_t+i qquad  st sum_i=-k^k omega_i = 1\n\nFor a MA of order p, the weights provided by the user have to be a vector of size (p x 1).","category":"section"},{"location":"man/moving/#Calculation-for-boundary-or-edge-values","page":"Moving Average","title":"Calculation for boundary or edge values","text":"When using a centered moving average of order p, for the first fracp-12 and the last fracp-12 values  of the time series there are not enought data points available. To generalise about these boundary cases, the normal centered MA can also be thought as a special case of weighted MA with uniform weights, where each weight must equal  w_i = frac1p. For k missing values, the weights that still can be applied are normalized so that they sum to 1, so that each of the (p-k) remaining weights now equals frac1p-k.  As an example, the calculation of the first value of a time series y using a centered MA of order 5 becomes  MA_1(5) = sum_i=1^3 frac13 y_i = frac13 sum_i=1^3 y_i.\n\nFor weighted MA with given weights w_1w_p, in case that for the first k weights there are no data points available for the  calculation, the remaining weights are normalized as follows w_i = fracw_isum_j=k+1^pw_j. Correspondingly, in case the last k weights cannot be used, the remaining p-k weights are normalized: w_i = fracw_isum_j=1^p-kw_j\n\nAs it is common to drop data for which the full order of the specified MA cannot be computed,  discard=true will return NaN[1] for each datum in the output vector, where not enough data points are available due to boundary cases.\n\n[1]: Be aware that Julia follows the IEEE 754 standard for floating-point values. The operation NaN == NaN will result in false, which makes e.g. comparisons  of vectors very tricky.","category":"section"},{"location":"man/moving/#Seasonal-Average","page":"Moving Average","title":"Seasonal Average","text":"For each season component S_i this function calculates the average for all observation that where recorded in the same type of season given 12s types.\n\nSince the formal mathematical notation requires the use of sets, the interested reader can see the formula for calculating the seasonal averages in the notes down below[2].\n\n[2]: For the time series y_1 y_2y_T of length T and the number of seasons s, each observation y_t t in 1T,  can belong only to one set mathcalS_i, i in 1s out of all possible s sets, where each set is comprised of all observations that take place in the same type of season.  Therefore the superset mathcalS, consisting of all individual disjoint sets mathcalS_i, can be defined as  mathcalS = bigcup_i = 1^s mathcalS_iwhere  mathcalS_i = y_t  ((t-1) mod s) + 1 = i and mathcalS_i cap mathcalS_j = emptyset for all i neq j. So the seasonal average for the i-th number of season S_i can be written and computed asS_i = frac1mathcalS_i sum_x in mathcalS_i x\n","category":"section"},{"location":"man/moving/#Classical-decomposition","page":"Moving Average","title":"Classical decomposition","text":"Given the implementations of maSeason and rollingAverage, the following two decompositions using moving averages can be achieved: \n\nThe additive model\n\n\ty_t = tau_t + S_t + u_t\n\nor the multiplicative model\n\n\ty_t = tau_t * S_t * u_t\n\nwhere tau_t is the trend component estimated using moving averages, S_t is the average season component estimated using the detrended time series and u_t is simply the residual factor defined as u_t = y_t - tau_t - S_t.","category":"section"},{"location":"man/moving/#TrendDecomposition.rollingAverage-Tuple{Vector, Int64}","page":"Moving Average","title":"TrendDecomposition.rollingAverage","text":"rollingAverage(y :: Vector, p :: Int; centered::Bool = true, discard::Bool = false, offset::Int = 0)\n\nComputes the moving average of the vector y with p data points.\n\nWith centered equal true, p has to be an odd number so that an equal number of leads and lags can be used. By default when centered = false, the computation includes the datum plus its most recent p-1 lagged values. This behavior can be changed by including an offset; a positive offset includes one additonal lead at the expense of the oldest lag value\n\nFor discard=false the function rolls over all data, even if not all p data points can be used.\n\n\n\n\n\n","category":"method"},{"location":"man/moving/#TrendDecomposition.rollingAverage-Tuple{Vector, Int64, Vector}","page":"Moving Average","title":"TrendDecomposition.rollingAverage","text":"rollingAverage(y :: Vector, p :: Int, weights :: Vector;\n    centered::Bool = true, discard::Bool = false, offset::Int = 0)\n\nComputes the moving average of the vector y with p data points weighted by vector w.\n\nWith centered equal true, p has to be an odd number so that an equal number of leads and lags can be used. By default when centered = false, the computation includes the datum plus its most recent p-1 lagged values. This behavior can be changed by including an offset; a positive offset includes one additonal lead at the expense of the oldest lag value.\n\nFor discard=false the function rolls over all data, even if not all p data points can be used, in any case all used weights will sum to 1.\n\n\n\n\n\n","category":"method"},{"location":"man/moving/#TrendDecomposition.maSeason","page":"Moving Average","title":"TrendDecomposition.maSeason","text":"maSeason(y :: Vector, seasons :: Int; repeating::Bool = false)\n\nGiven the number of seasons, the function computes the average value of each season component. This method works better for detrended data.\n\nWith repeating equal true the function will repeat the results until the output vector has the same length as the input vector y.\n\n\n\n\n\n","category":"function"},{"location":"man/moving/#TrendDecomposition.maDecompose","page":"Moving Average","title":"TrendDecomposition.maDecompose","text":"maDecompose(y :: Vector, seasons :: Int; combine::Bool = false, model=:add)\n\nDecomposes the time series y into trend (T), season (S) and noise components (I).\n\nEither assumes an additive model (:add) or a multiplicative model (:mul).\n\nReplicates the R decompose{stats} function. For a more generic function implementation see decompose of this package (TrendDecomposition.decompose).\n\nReturns a matrix where the columns correspont to the above mentioned components in (T, S, I) order; with combine equal true it returns (y, T, S, I) as columns.\n\n\n\n\n\n","category":"function"},{"location":"#TrendDecomposition.jl","page":"Introduction","title":"TrendDecomposition.jl","text":"Welcome to the TrendDecomposition.jl documentation.\n\ninfo: Info\nThis is a preliminary version of the documentation. The package is also not feature complete until version 1.0, thus sometimes there are references to not yet implemented features.\n\nTrendDecomposition.jl is a Julia package for the decomposition of time series into trend and cycle components. More generally it provides  both (stochastic) trend component estimation and forecasting, though not all methods are suitable for forecasting.\n\nBy using filters and smoothers the most pragmatic approach to trend decomposition is estimating the trend t and defining the cyclical component c of time series y as c = y - t. Often it is up to the user of this module to calculate the cyclical components themselves with the computed trend returned from a function  provided by this module.\n\nThe following is a list of already implemented and documented methods:\n\nExponential Smoothing\nSimple exponential smoothing\nDouble exponential smoothing / Brown linear method\nHolt Linear procedure\nHolt Winters method\nPenalized smoothing\nBohlmann Filter / Whittaker-Henderson Smoothing\nLeser / Hodrick-Prescott (HP) Filter\nBoosted HP Filter\nTaut string - piecewise constant / Total variation denoising\nL1 trend filtering with ADMM\nL1 trend filtering with ADMM using taut string\nL1 trend filtering with ADMM using fused Lasso\nMoving Average (MA)\nSeasonal Average\nClassical Decomposition by moving averages\n\nExamples, which demonstrate the usecase of some of the implemented methods, can be found in the Usage section. ","category":"section"},{"location":"man/api/#Functions","page":"Index","title":"Functions","text":"","category":"section"},{"location":"man/misc/#Additional-Functions","page":"Miscellaneous","title":"Additional Functions","text":"","category":"section"},{"location":"man/misc/#Useful-or-Helper-Functions","page":"Miscellaneous","title":"Useful or Helper Functions","text":"Here are functions that have a usecase in other areas that fall not strictlly into any  offical categories for trend estimation.","category":"section"},{"location":"man/misc/#Experimental","page":"Miscellaneous","title":"Experimental","text":"Here are functions that are only experimental in use but could be useful for some specific purposes.","category":"section"},{"location":"man/misc/#TrendDecomposition.greatestConvexMinorant","page":"Miscellaneous","title":"TrendDecomposition.greatestConvexMinorant","text":"greatestConvexMinorant(y :: Vector)\n\nComputes greatest convex minorant of series y using the pool-adjecent-violators algorithm\n\nReturns the coordinates of the knots as a tuple (x, y) \n\n\n\n\n\n","category":"function"},{"location":"man/misc/#TrendDecomposition.leastConcaveMajorant-Tuple{Vector}","page":"Miscellaneous","title":"TrendDecomposition.leastConcaveMajorant","text":"leastConcaveMajorant(y :: Vector)\n\nComputes least concave majorant (lcm) of series y using the pool-adjecent-violators algorithm\n\nReturns the coordinates of the knots as a tuple (x, y) \n\n\n\n\n\n","category":"method"},{"location":"man/misc/#TrendDecomposition.trendL1Filter","page":"Miscellaneous","title":"TrendDecomposition.trendL1Filter","text":"trendL1Filter(y :: Vector, λ :: Real; m = 2, max_iter=20, method = :ADMM)\n\nPlaceholder for trendL1Filter extension, when using TrendDecomposition together with other Julia packages like Convex.jl and SCS.jl.\n\nThis function provides the generic use of serveral optimization methods to compute a numerical solution. Following methods are implmented: :ADMM -> alternating direction method of multipliers :ConvexSCS -> SCS solver with Convex.jl. Prerequisite! Import necessary modules with: using Convex, SCS\n\nThe function returns the estimated trend component\n\n\n\n\n\n","category":"function"}]
}
